# ELK(Docker) via Ansible

Simple utility to deploy ELK-Stack and Filebeat in docker containers 
Used technologies:
- Ansible
- Docker, Docker-Compose
- Elasticsearch, Logstash, Kibana, Filebeat

## Project Structure


```
ansible-elk
├── docker
│   ├── defaults
│   │   └── main.yml
│   ├── handlers
│   │   └── main.yml
│   ├── tasks
│   │   ├── docker-compose.yml
│   │   ├── docker-https.yml
│   │   ├── main.yml
│   │   └── pip-install.yml
│   └── vars
│       └── main.yml
├── docker-compose.yml
├── elk
│   ├── elasticsearch
│   │   ├── config
│   │   │   └── elasticsearch.yml
│   │   └── Dockerfile
│   ├── kibana
│   │   ├── config
│   │   │   └── kibana.yml
│   │   └── Dockerfile
│   ├── logstash
│   │   ├── config
│   │   │   └── logstash.yml
│   │   ├── Dockerfile
│   │   └── pipeline
│   │       └── logstash.conf
│   ├── setup
│   │   ├── Dockerfile
│   │   ├── entrypoint.sh
│   │   ├── helpers.sh
│   │   └── roles
│   │       └── logstash_writer.json
│   └── tasks
│       └── main.yml
├── hosts
├── README.md
└── start.yml
```
### docker
Folder that is used to store components for Docker and Docker-compose install with Ansible.
- To change version of docker navigate to the `docker/defaults/main.yml` file.
- To add necessary packages check the `docker/vars/main.yml` file.
- All auxiliary tasks are included in the `docker/tasks/main.yml` file.
___
### docker-compose
File that is defining services, networks, and volumes for a Docker ELK application. 
**Consists of:**
1. **Setup**: runs a one-off script which initializes users inside Elasticsearch with the values of the passwords defined in the [`.env`](.env)  file.
1. **Elasticsearch**: starts Elasticsearch docker container according to the configuration: \
*Ports:*  9200: Elasticsearch HTTP | 9300: Elasticsearch TCP transport \
*Configuration file*: elasticsearch.yml \
*Volume*: elasticsearch (persists data generated by elasticsearch container to `/var/lib/docker/volumes` folder)
1. **Kibana**: starts Kibana docker container according to the configuration: \
*Ports:*  5601: Kibana \
*Configuration file*: kibana.yml  \
1. **Logstash**: starts Logstash docker container according to the configuration: \
*Ports:*  5044: Logstash Beats input | 5000: Logstash TCP/UDP input | 9600: Logstash monitoring API \
*Configuration file*: logstash.yml \
*Pipeline file*: logstash.conf (specifies on which port to receive packets and the port of service on which to send after optional filtering)
___
### hosts / start.yml
*Hosts* is an ansible inventory file, which specifies the IP of the servers where the ELK-Stack will be deployed `ELK_Master`  and client with Filebeat `Filebeat_Slave`. \
Check the inventory file.
```
ansible-inventory -i ./hosts --list
```
*start.yml* is an ansible playbook file, which is executing the defined tasks on the targeted `hosts`

___
### elk
Directory that stores all data about ELK such as Dockerfiles, configuration and pipeline files. Follow this [link](https://github.com/docker-library/repo-info/tree/master/repos) to check if new version of ELK is available. To change version of ELK copy digest like `kibana@sha256:e526855c6c7207d5de504d97accce0b0552c74e20ef08b0abcb17c918bd6d070` [here](https://github.com/docker-library/repo-info/blob/master/repos/kibana/remote/8.2.2.md) and replace old in Dockerfile.


The deployment procedure of ELK is described in the `elk/tasks/main.yml` file
___
## Installation


Clone repository to your machine and run ansible-playbook.

```sh
git clone https://gitlab.wltm.ru/vjacheslav.gordienko/ansible-elk.git
cd ansible-elk
ansible-playbook -i ./hosts start.yml
```
Upon the initial startup, the `elastic`, `logstash_internal` and `kibana_system` Elasticsearch users are intialized with the values of the passwords defined in the [`.env`](.env) file ("changeme" by default). The first one is the built-in superuser, the other two are used by Kibana and Logstash respectively to communicate with Elasticsearch. This task is only performed during the initial startup of the stack. 

Give **Kibana** about a minute to initialize, then access the Kibana web UI by opening http://IPOFSERVER:5601 in a web browser and use the following (default) credentials to log in:
* user: *elastic*
* password: *changeme*
### Initial setup
The _"changeme"_ password set by default for all aforementioned users is **unsecure**. For increased security, you can
reset the passwords of all aforementioned Elasticsearch users to random secrets.

1. Reset passwords for default users

    The commands below resets the passwords of the `elastic`, `logstash_internal` and `kibana_system` users. Take note
    of them.

    ```sh
    docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user elastic
    ```

    ```sh
    docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user logstash_internal
    ```

    ```sh
    docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user kibana_system
    ```

1. Replace usernames and passwords in configuration files

    Replace the password of the `elastic` user inside the `.env` file with the password generated in the previous step.
    Its value isn't used by any core component, but extensions use it to
    connect to Elasticsearch.

    > **Note**  
    > In case you don't plan on using any of the provided extensions, or
    > prefer to create your own roles and users to authenticate these services, it is safe to remove the
    > `ELASTIC_PASSWORD` entry from the `.env` file altogether after the stack has been initialized.

    Replace the password of the `logstash_internal` user inside the `.env` file with the password generated in the
    previous step. Its value is referenced inside the Logstash pipeline file (`elk/logstash/pipeline/logstash.conf`).

    Replace the password of the `kibana_system` user inside the `.env` file with the password generated in the previous
    step. Its value is referenced inside the Kibana configuration file (`elk/kibana/config/kibana.yml`).

1. Restart Logstash and Kibana to re-connect to Elasticsearch using the new passwords

    ```sh
    docker-compose up -d logstash kibana
    ```
    
### Injecting data

Open the Kibana web UI by opening <http://IPOFSERVER:5601> in a web browser and use the following credentials to log in:

* user: *elastic*
* password: *changeme*

Now that the stack is fully configured, you can go ahead and inject some log entries. The shipped Logstash configuration
allows you to send content via TCP:

```sh
# Using BSD netcat (Debian, Ubuntu, MacOS system, ...)
cat /path/to/logfile.log | nc -q0 localhost 5000
echo "message" | nc -u localhost 5000
```


### Useful commands
To start all docker containers manually use:
```sh
docker-compose up
```

In order to entirely shutdown the stack and remove all persisted data, use the following Docker Compose command:

```sh
docker-compose down -v
```




